This section proposes possible improvements to models, data and further experiments to conduct. 

\subsection{Unsupervised Training on Remaining DRE19 Data} \label{sec:fwork1}
Approximately 15.000 images out of the 22.000 have not been labeled or sorted. 
With the experiments conducted in this paper, it appears that the unsupervised learning strategy provides promising results for transfer learned autoencoders.
A natural next step would be to add a portion of the remaining data to training data and keep a much larger validation and test set for evaluation.
Doing so could potentially push an autoencoder towards learning more accurate and denser representations. 
\newline
\newline
In addition, given the \~95\% top-5 accuracy in the final VGG16-AE-512 model, it may be feasible to construct an overlapping prediction scheme for labeling new images.
This could be achieved by for any new image, predict e.g. the 5 most similar items from the labeled dataset and produce a class label by means of e.g. a KNN-classifier for the new image based on the most prevalent predicted class:
$$
Image \xrightarrow{\text{KNN(top5(VGG16-AE-512))}} \hat{y}_{5} = [bathroom, bathroom, entre, bathroom, kitchen]
$$
$$
\xrightarrow{\text{$max_{c}(p)$}}  \hat{y} = bathroom 
$$
\newline
Furthermore it would be interesting to see how the model behaves given data that was deliberately excluded from the curated DRE19 data. 
An analysis of may show that the model lacks robustness or that it does in fact produce large cosine distance coefficients for e.g. gardens and bathrooms, such that these could be excluded from predictions to potential end-users.

\subsection{Further Experimentation with Autoencoders}
The concept of Convolutional Autoencoders has not been touched on in this paper. 
This variation on autoencoders have proven useful in feature extraction from images\autocite{MAGGIPINTO2018126} in other domains.
Such autoencoders consist of convolutional layers, that are much more adept and handling higher-dimensional data and are very perfomant in other computer vision tasks.
Using a convolutional input layer, an autoencoder could also directly transfer input from earlier, higher-dimensional layers in pretrained models, which may further boost the learning of meaningful features. 
\newline
\newline
Lastly, given more time, as mentioned in \nameref{hellosec}, training models with the ADAM optimizer may yield faster convergence, allowing for quicker experimentation - and larger datasets (as proposed in \ref{sec:fwork1}).

\subsection{Prediction Speed and Applications}
The aim of this report has been to investigate the possibility of improving the homeseeking experience for customers in the danish real estate market. 
A thorough analysis of the runtime of the proposed architecture should be conducted in order to fully assess the success of the preliminary results obtained in this report in terms of potential applications.
\newline
\newline
A performant data structure for persisting and updating similarity calculations should also be investigated, as new homes enter the market constantly. 
In-line with the objective of this paper, a survey of methodologies for averaging multiple image-to-image predictions should be conducted, as to ascertain suitable techniques for producing multi-image-to-home predictions.




