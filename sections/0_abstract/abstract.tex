This paper investigates ability of autoencoders to learn meaningful hidden representations from publicly available pretrained neural networks.
Several variations of autoencoders have are trained in an unsupervised manner to reproduce images collected from the Danish online housing market, and show adeptness in distinguishing between six different types of rooms.
\newline
\newline
A thorough set of experiments are conducted on the data, showing that some pretrained networks produce decent predictions out-of-the-box, and that autoencoders can be trained to reduce the dimensionality by a factor of 8 from pretrained feature vectors while improving accuracy in four out of four classification accuracy metrics.
\newline
Methodologies for computing distance coefficients and retrieving similar images given a query image are explored and used in an exposition the capabilities of pretrained neural networks in comparison to various autoencoder architectures.
\newline
\newline
Finally, it is concluded 


